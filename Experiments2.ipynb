{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import *\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import cv2\n",
    "from torchmeta.modules import *\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchmeta.utils.data import *\n",
    "import random\n",
    "import maml\n",
    "from maml.metalearners import ModelAgnosticMetaLearning\n",
    "from torchmeta.utils.data import MetaDataset\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README.txt\n",
    "\n",
    "This folder contains the preliminary dataset for the coursework for R250: Applications of ML to Psychiatry.\n",
    "\n",
    "COBRE_demographics.csv includes a list of subject ids, labels, sex and age information.\n",
    "Labels: -1 for a healthy control subject, 1 for a patient with schizophrenia\n",
    "Sex: -1 for male, 1 for female\n",
    "PANSS total: these are symptom scores (from the Positive and Negative Syndrome Scale). Note controls do not have symptom data. Symptoms are also missing for 2 patients.\n",
    "\n",
    "COBRE_cortical_thickness.csv includes cortical thickness measurements for all subjects, for 308 cortical brain regions.\n",
    "\n",
    "COBRE_fmri_connectivity.mat is a Matlab file with connectivity data for all subjects. Subjects are listed in the same order as the demographics file above.\n",
    "Note that not only 293 out of 308 brain regions have fmri data available (due to incomplete fMRI coverage of the brain- some regions have signal drop out). These 293 regions are listed in the file COBRE_fmri_regions.csv.\n",
    "Also note that the values from the connectivity matrices are provided in vector format. To transform back to a connectivity matrix, you need to reshape the vectors.\n",
    "E.g. in Matlab, use the command: matrix = reshape(connectivity_data(ind,:),[293,293])\n",
    "where ind is an integer between 1 and 148 denoting the subject you are interested in. To check you have reshaped the matrix correctly, try plotting it- it should be a symmetric matrix with 1s on the diagonal.\n",
    "\n",
    "COBRE_fmri_connectivity.csv- as above but saved as a .csv file.\n",
    "\n",
    "COBRE_fmri_regions.csv- list of regions with fMRI data available (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = pd.read_csv(\"./data/COBRE_demographics.csv\", sep=r'\\s*,\\s*', engine=\"python\")\n",
    "fmri_connectivity = pd.read_csv(\"./data/COBRE_fmri_connectivity.csv\", sep=r'\\s*,\\s*', engine=\"python\")\n",
    "cortical_thickness = pd.read_csv(\"./data/COBRE_cortical_thickness.csv\", sep=r'\\s*,\\s*', engine=\"python\")\n",
    "fmri_regions = pd.read_csv(\"./data/COBRE_fmri_regions.csv\", sep=r'\\s*,\\s*', engine=\"python\")\n",
    "fmri_connectivity_np = genfromtxt(\"./data/COBRE_fmri_connectivity.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_regions = np.concatenate((np.array([0]), fmri_regions['Region_no'].to_numpy() + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick data investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 13, 14, 10]\n",
      "[20, 6, 6, 12]\n",
      "[6, 6, 6, 3]\n",
      "[1, 4, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "ages = demographics[\"age\"]\n",
    "\n",
    "c_pos_m = [0, 0, 0, 0]\n",
    "c_con_m = [0, 0, 0, 0]\n",
    "\n",
    "c_pos_f = [0, 0, 0, 0]\n",
    "c_con_f = [0, 0, 0, 0]\n",
    "\n",
    "for i in range(len(demographics[\"age\"])):\n",
    "    \n",
    "    t_pos_m = 0\n",
    "    t_neg_m = 0\n",
    "    \n",
    "    t_pos_f = 0\n",
    "    t_neg_f = 0\n",
    "    \n",
    "    if(demographics[\"labels\"][i] == -1):\n",
    "        if(demographics[\"sex\"][i] == -1):\n",
    "            t_pos_m = 1\n",
    "        else:\n",
    "            t_pos_f = 1\n",
    "    else:\n",
    "        if(demographics[\"sex\"][i] == -1):\n",
    "            t_neg_m = 1\n",
    "        else:\n",
    "            t_neg_f = 1\n",
    "            \n",
    "    for j in range(4):\n",
    "        min_age = (j+2)*10\n",
    "        max_age = min_age + 10\n",
    "        if(demographics[\"age\"][i] > min_age and demographics[\"age\"][i] < max_age):\n",
    "            c_pos_m[j] += t_pos_m\n",
    "            c_con_m[j] += t_neg_m\n",
    "            c_pos_f[j] += t_pos_f\n",
    "            c_con_f[j] += t_neg_f\n",
    "         \n",
    "            \n",
    "print(c_pos_m)\n",
    "print(c_con_m)\n",
    "\n",
    "print(c_pos_f)\n",
    "print(c_con_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_to_data = {} # Each item to a tuple (Age, True / False, fMRI_connectivity_matrix, cortical_thickness)\n",
    "for p in range(len(demographics[\"SubjectID\"])):\n",
    "    patient = demographics.iloc[p]\n",
    "    p_fmri = fmri_connectivity_np[p, :]\n",
    "    if(patient[\"sex\"] != -1):\n",
    "        continue \n",
    "    subject_to_data[patient[\"SubjectID\"]] = [patient[\"age\"], (patient[\"labels\"] == -1), np.reshape(p_fmri, (293, 293)), np.zeros(308)]\n",
    "    \n",
    "for p in range(len(cortical_thickness[\"Subject_ID\"])):\n",
    "    patient = cortical_thickness.iloc[p]\n",
    "    if(patient[\"Subject_ID\"] not in subject_to_data):\n",
    "        continue\n",
    "    \n",
    "    subject_to_data[patient[\"Subject_ID\"]][3] = patient.to_numpy()[2:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 256, 256) (102,)\n",
      "(12, 256, 256) (12,)\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "for pid, l in subject_to_data.items():\n",
    "    x_temp = cv2.resize(l[2]*255, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(x_temp)\n",
    "    y.append(int(l[1]))\n",
    "\n",
    "X = np.array(X).astype(np.double)\n",
    "y = np.array(y).astype(np.double)\n",
    "    \n",
    "train_x, val_x, train_y, val_y = train_test_split(X, y, test_size = 0.1)\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(val_x.shape, val_y.shape)\n",
    "\n",
    "train_x = train_x.reshape(102,1, 256, 256)\n",
    "train_x  = torch.from_numpy(train_x)\n",
    "val_x = val_x.reshape(12, 1, 256, 256)\n",
    "val_x  = torch.from_numpy(val_x)\n",
    "\n",
    "val_y = val_y.astype(int)\n",
    "val_y = torch.from_numpy(val_y)\n",
    "train_y = train_y.astype(int)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 1, 256, 256)\n",
      "(19,)\n",
      "(3, 1, 256, 256)\n",
      "(3,)\n",
      "\n",
      "(24, 1, 256, 256)\n",
      "(24,)\n",
      "(3, 1, 256, 256)\n",
      "(3,)\n",
      "\n",
      "(36, 1, 256, 256)\n",
      "(36,)\n",
      "(4, 1, 256, 256)\n",
      "(4,)\n",
      "\n",
      "(19, 1, 256, 256)\n",
      "(19,)\n",
      "(3, 1, 256, 256)\n",
      "(3,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-c28c1580e505>:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  l = np.array(d[cat])\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for pid, l in subject_to_data.items():\n",
    "    x_temp = cv2.resize(l[2]*255, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
    "    cat = (int(l[0] / 10)) - 2\n",
    "    if cat not in [0, 1, 2, 3]:\n",
    "        continue\n",
    "    \n",
    "    if cat not in d:\n",
    "        d[cat] = []\n",
    "    \n",
    "    d[cat].append([int(l[1]), x_temp])\n",
    " \n",
    "e = {}\n",
    "for cat in d:\n",
    "    l = np.array(d[cat])\n",
    "    \n",
    "    train_x2 = np.zeros(((math.ceil(.9* l[:, 1].shape[0])), l[:, 1][0].shape[0], l[:, 1][0].shape[1]))\n",
    "    for i, item in enumerate(l[:, 1]):\n",
    "        if(i >= train_x2.shape[0]):\n",
    "            break\n",
    "        train_x2[i, :, :] = item\n",
    "    \n",
    "    train_y2 = np.array(l[:train_x2.shape[0], 0], dtype=np.int16)\n",
    "    train_x2 = np.expand_dims(train_x2, axis=1)\n",
    "    print(train_x2.shape) \n",
    "    print(train_y2.shape)\n",
    "    \n",
    "    test_x2 = np.zeros((math.ceil(.1* l[:, 1].shape[0]), l[:, 1][0].shape[0], l[:, 1][0].shape[1]))\n",
    "    c = 0\n",
    "    for i, item in reversed(list(enumerate(l[:, 1]))):\n",
    "        if(c >= test_x2.shape[0]):\n",
    "            break\n",
    "        test_x2[c, :, :] = item\n",
    "        c+=1\n",
    "    test_y2 = np.array(l[-1* test_x2.shape[0]:, 0], dtype=np.int16)\n",
    "    test_x2 = np.expand_dims(test_x2, axis=1)\n",
    "    print(test_x2.shape)\n",
    "    print(test_y2.shape)\n",
    "    print()\n",
    "    e[cat] = (train_x2, train_y2, test_x2, test_y2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n",
      "(0.6068181818181818, 0.1554950771190666)\n",
      "20 to 30\n",
      "(0.6666666666666667, 0.15811388300841897)\n",
      "30 to 40\n",
      "(0.6666666666666666, 0.31622776601683794)\n",
      "40 to 50\n",
      "(0.4333333333333333, 0.3511884584284246)\n",
      "50 to 60\n",
      "(0.6333333333333333, 0.4068851871911235)\n"
     ]
    }
   ],
   "source": [
    "# Without doing k-means classification\n",
    "\n",
    "def get_results(subject_to_data, min_age, max_age):\n",
    "    X = []\n",
    "    y = []\n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    for pid, l in subject_to_data.items():\n",
    "        if(l[0] >= min_age and l[0] < max_age):\n",
    "            X.append(l[2].flatten())\n",
    "            #X.append(l[3])\n",
    "            y.append(int(l[1]))\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    acc = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        svclassifier = SVC(kernel='linear')\n",
    "        svclassifier.fit(X_train, y_train)\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    acc = np.array(acc)\n",
    "    return (np.mean(acc), np.std(acc))\n",
    "    #print(confusion_matrix(y_test,y_pred))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(\"All\")    \n",
    "print(get_results(subject_to_data, 0, 100))\n",
    "\n",
    "print(\"20 to 30\")    \n",
    "print(get_results(subject_to_data, 20, 30))\n",
    "\n",
    "print(\"30 to 40\")    \n",
    "print(get_results(subject_to_data, 30, 40))\n",
    "\n",
    "print(\"40 to 50\")    \n",
    "print(get_results(subject_to_data, 40, 50))\n",
    "\n",
    "print(\"50 to 60\")    \n",
    "print(get_results(subject_to_data, 50, 60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â K-means clustering\n",
    "def k_means_get_results(subject_to_data, min_age, max_age):\n",
    "    X = []\n",
    "    y = []\n",
    "    for pid, l in subject_to_data.items():\n",
    "        if(l[0] >= min_age and l[0] < max_age):\n",
    "            #X.append(l[2].flatten())\n",
    "            X.append(l[3])\n",
    "            y.append(int(l[1]))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    kmeans = GaussianMixture(n_components=5, covariance_type='diag').fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    b2 = kmeans.predict_proba(X)\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=False)\n",
    "    kf.get_n_splits(b2)\n",
    "    \n",
    "    acc = []\n",
    "    for train_index, test_index in kf.split(b2):\n",
    "        X_train, X_test = b2[train_index], b2[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        svclassifier = SVC(kernel='linear')\n",
    "        svclassifier.fit(X_train, y_train)\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    acc = np.array(acc)\n",
    "    return (np.mean(acc), np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n",
      "(0.5856060606060606, 0.08886915816371432)\n",
      "20 to 30\n",
      "(0.7416666666666667, 0.16007810593582122)\n",
      "30 to 40\n",
      "(0.5333333333333333, 0.3559026084010437)\n",
      "40 to 50\n",
      "(0.55, 0.4153311931459037)\n",
      "50 to 60\n",
      "(0.35, 0.26299556396765833)\n"
     ]
    }
   ],
   "source": [
    "print(\"All\")    \n",
    "print(k_means_get_results(subject_to_data, 0, 100))\n",
    "\n",
    "print(\"20 to 30\")    \n",
    "print(k_means_get_results(subject_to_data, 20, 30))\n",
    "\n",
    "print(\"30 to 40\")    \n",
    "print(k_means_get_results(subject_to_data, 30, 40))\n",
    "\n",
    "print(\"40 to 50\")    \n",
    "print(k_means_get_results(subject_to_data, 40, 50))\n",
    "\n",
    "print(\"50 to 60\")    \n",
    "print(k_means_get_results(subject_to_data, 50, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE before k-means clustering\n",
    "def tsne_k_means_get_results(subject_to_data, min_age, max_age):\n",
    "    X = []\n",
    "    y = []\n",
    "    for pid, l in subject_to_data.items():\n",
    "        if(l[0] >= min_age and l[0] < max_age):\n",
    "            X.append(l[2].flatten())\n",
    "            #X.append(l[3])\n",
    "            y.append(int(l[1]))\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    X = TSNE(n_components=2).fit_transform(X)\n",
    "    \n",
    "    kmeans = GaussianMixture(n_components=5, covariance_type='diag').fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    b2 = kmeans.predict_proba(X)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    kf.get_n_splits(b2)\n",
    "    \n",
    "    acc = []\n",
    "    for train_index, test_index in kf.split(b2):\n",
    "        X_train, X_test = b2[train_index], b2[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        svclassifier = SVC(kernel='linear')\n",
    "        svclassifier.fit(X_train, y_train)\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    acc = np.array(acc)\n",
    "    return (np.mean(acc), np.std(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n",
      "(0.6573122529644269, 0.09102587446466129)\n",
      "20 to 30\n",
      "(0.6666666666666667, 0.22360679774997894)\n",
      "30 to 40\n",
      "(0.4, 0.43588989435406733)\n",
      "40 to 50\n",
      "(0.41666666666666663, 0.30956959368344517)\n",
      "50 to 60\n",
      "(0.3833333333333333, 0.27938424357067015)\n"
     ]
    }
   ],
   "source": [
    "print(\"All\")    \n",
    "print(tsne_k_means_get_results(subject_to_data, 0, 100))\n",
    "\n",
    "print(\"20 to 30\")    \n",
    "print(k_means_get_results(subject_to_data, 20, 30))\n",
    "\n",
    "print(\"30 to 40\")    \n",
    "print(k_means_get_results(subject_to_data, 30, 40))\n",
    "\n",
    "print(\"40 to 50\")    \n",
    "print(k_means_get_results(subject_to_data, 40, 50))\n",
    "\n",
    "print(\"50 to 60\")    \n",
    "print(k_means_get_results(subject_to_data, 50, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(Module):\n",
    "    def __init__(self):\n",
    "        super(ConvModel, self).__init__()\n",
    "        self.a1 = Conv2d(1, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.a2 =BatchNorm2d(4)\n",
    "        self.a3 = ReLU(inplace=True)\n",
    "        self.a4 = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.a5 = Conv2d(4, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.a6 = BatchNorm2d(4)\n",
    "        self.a7 = ReLU(inplace=True)\n",
    "        self.a8 = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(4 * 64 * 64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.a6(x)\n",
    "        x = self.a7(x)\n",
    "        x = self.a8(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = Variable(train_x.double()), Variable(train_y.long())\n",
    "    #x_train = torch.from_numpy(x_train)\n",
    "    #y_train = torch.from_numpy(y_train)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = Variable(val_x), Variable(val_y)\n",
    "    # converting the data into GPU format\n",
    "    \n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train.double())\n",
    "    output_val = model(x_val.double())\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train.long())\n",
    "    loss_val = criterion(output_val, y_val.long())\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvModel(\n",
      "  (a1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (a2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (a3): ReLU(inplace=True)\n",
      "  (a4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (a5): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (a6): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (a7): ReLU(inplace=True)\n",
      "  (a8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch :  1 \t loss : tensor(0.8709, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  3 \t loss : tensor(420.6550, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  5 \t loss : tensor(29.0627, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  7 \t loss : tensor(34.8380, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  9 \t loss : tensor(5.6889, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  11 \t loss : tensor(16.5751, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  13 \t loss : tensor(12.5080, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  15 \t loss : tensor(4.4833, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  17 \t loss : tensor(0.6782, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  19 \t loss : tensor(0.7275, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  21 \t loss : tensor(0.6802, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  23 \t loss : tensor(0.6794, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  25 \t loss : tensor(0.6793, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  27 \t loss : tensor(0.6793, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  29 \t loss : tensor(0.6795, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  31 \t loss : tensor(0.6799, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  33 \t loss : tensor(0.6807, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  35 \t loss : tensor(0.6852, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  37 \t loss : tensor(0.6989, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  39 \t loss : tensor(0.7107, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  41 \t loss : tensor(0.6993, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  43 \t loss : tensor(0.7009, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  45 \t loss : tensor(0.7074, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  47 \t loss : tensor(0.7128, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  49 \t loss : tensor(0.7179, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  51 \t loss : tensor(0.7218, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  53 \t loss : tensor(0.7238, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  55 \t loss : tensor(0.7240, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  57 \t loss : tensor(0.7227, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  59 \t loss : tensor(0.7203, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  61 \t loss : tensor(0.7171, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  63 \t loss : tensor(0.7135, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  65 \t loss : tensor(0.7099, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  67 \t loss : tensor(0.7066, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  69 \t loss : tensor(0.7042, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  71 \t loss : tensor(0.7025, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  73 \t loss : tensor(0.7010, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  75 \t loss : tensor(0.7001, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  77 \t loss : tensor(0.7014, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  79 \t loss : tensor(0.7039, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  81 \t loss : tensor(0.7040, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  83 \t loss : tensor(0.7058, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  85 \t loss : tensor(0.7087, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  87 \t loss : tensor(0.7116, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  89 \t loss : tensor(0.7124, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  91 \t loss : tensor(0.7147, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  93 \t loss : tensor(0.7174, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  95 \t loss : tensor(0.7203, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  97 \t loss : tensor(0.7240, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  99 \t loss : tensor(0.7266, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  101 \t loss : tensor(0.7308, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  103 \t loss : tensor(0.7339, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  105 \t loss : tensor(0.7375, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  107 \t loss : tensor(0.7414, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  109 \t loss : tensor(0.7461, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  111 \t loss : tensor(0.7491, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  113 \t loss : tensor(0.7528, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  115 \t loss : tensor(0.7577, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  117 \t loss : tensor(0.7612, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  119 \t loss : tensor(0.7644, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  121 \t loss : tensor(0.7690, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  123 \t loss : tensor(0.7727, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  125 \t loss : tensor(0.7757, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  127 \t loss : tensor(0.7796, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  129 \t loss : tensor(0.7830, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  131 \t loss : tensor(0.7858, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  133 \t loss : tensor(0.7894, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  135 \t loss : tensor(0.7924, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  137 \t loss : tensor(0.7950, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  139 \t loss : tensor(0.7985, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  141 \t loss : tensor(0.8013, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  143 \t loss : tensor(0.8037, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  145 \t loss : tensor(0.8069, dtype=torch.float64, grad_fn=<NllLossBackward>)\n",
      "Epoch :  147 \t loss : tensor(0.8089, dtype=torch.float64, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = ConvModel()\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "criterion = CrossEntropyLoss()\n",
    "model = model.double()  \n",
    "print(model)\n",
    "# defining the number of epochs\n",
    "n_epochs = 300\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    \n",
    "# plotting the training and validation loss\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(train_x.cpu().double())\n",
    "    \n",
    "softmax = torch.exp(output.cpu())\n",
    "prob = softmax.numpy()\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "print(accuracy_score(train_y, predictions))\n",
    "\n",
    "#with torch.no_grad():\n",
    "#    output = model(val_x.cpu().double())\n",
    "\n",
    "#softmax = torch.exp(output).cpu()\n",
    "#prob = softmax.numpy()\n",
    "#predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on validation set\n",
    "#print(accuracy_score(val_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Agnostic Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaConvModel(MetaModule):\n",
    "    def __init__(self):\n",
    "        super(MetaConvModel, self).__init__()\n",
    "        self.a1 = MetaConv2d(1, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.a2 = MetaBatchNorm2d(4)\n",
    "        self.a3 = ReLU(inplace=True)\n",
    "        self.a4 = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.a5 = MetaConv2d(4, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.a6 = MetaBatchNorm2d(4)\n",
    "        self.a7 = ReLU(inplace=True)\n",
    "        self.a8 = MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.linear_layers = MetaSequential(\n",
    "            MetaLinear(4 * 64 * 64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.a1(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.a6(x)\n",
    "        x = self.a7(x)\n",
    "        x = self.a8(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MetaConvModel()\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "metalearner = ModelAgnosticMetaLearning(model,\n",
    "                            optimizer,\n",
    "                            first_order=False,\n",
    "                            num_adaptation_steps=1,\n",
    "                            step_size=.1,\n",
    "                            loss_function=criterion,\n",
    "                            device=\"cpu\")\n",
    "\n",
    "r = []\n",
    "inner_losses = []\n",
    "outer_losses = []\n",
    "mean_outer_losses = []\n",
    "accuracies_before = []\n",
    "accuracies_after = []\n",
    "\n",
    "for epoch in range(300):\n",
    "    results = metalearner.train(e)\n",
    "    #print(results)\n",
    "    r.append(results)\n",
    "    inner_losses.append(results[\"inner_losses\"])\n",
    "    outer_losses.append(results[\"outer_losses\"])\n",
    "    mean_outer_losses.append(results[\"mean_outer_loss\"])\n",
    "    accuracies_before.append(results[\"accuracies_before\"])\n",
    "    accuracies_after.append(results[\"accuracies_after\"])\n",
    "#results = metalearner.evaluate(val_x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(mean_outer_losses)\n",
    "aaf = np.array(accuracies_after)\n",
    "#print(aaf)\n",
    "means = np.mean(aaf, axis=1)\n",
    "#print(means)\n",
    "m = np.convolve(means, np.ones(2)/2, mode='valid')\n",
    "plt.plot(range(len(means)), means)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(mean_outer_losses)\n",
    "#ols = np.array(outer_losses)\n",
    "#print(aaf)\n",
    "ols = np.mean(outer_losses, axis=1)\n",
    "#print(means)\n",
    "plt.plot(range(len(ols)), ols)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
